---
title: "Creating a test data set"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
```

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(maps)
library(mapdata)
```

## What do our samples look like?
Read in data.

There were two main sequencing efforts by RTL and GW. Chase says we should be suspicious of the RTL set.

There are therefore duplicated sequences.

```{r, warning=FALSE, message=FALSE}
data<-read_csv(file="./metadata/190828_Pike ddRAD data identifiers.csv")
data<-data %>% separate(`Coordinates (if available)`, into=c("Latitude", "Longitude"), sep =",")
data$Latitude<-as.numeric(data$Latitude)
data$Longitude<-as.numeric(data$Longitude)

dups<-data %>% select(`Tissue identifier`) %>% group_by(`Tissue identifier`) %>% summarize(Count=n()) %>% filter (Count > 1)
sum(dups$Count)

#Make a df without the dups
dedup<- filter(data, !(`Tissue identifier` %in% dups$`Tissue identifier`))

temp<-filter(data, `Tissue identifier` %in% dups$`Tissue identifier`) %>% arrange(`DNA identifier`)

#Apparently we may not want the RTL files
temp2 <- temp %>% filter(`Sequencing project` !="RTL 6997240")

#And some are duplicated as "topoff"" files
temp3 <- temp2 %>% filter(!grepl("topoff",`Sequencing file name`))

#Recombine the df without dups and the temp file.
data<-rbind(dedup, temp3)

```



```{r, warning=FALSE, message=FALSE}
nrow(data)
#Are there sames that are not duplicated from the RTL sequencing effort?
RTL<-filter(data, `Sequencing project`=="RTL 6997240")
write_csv(RTL, path="outputs/101/RTL-unique-samples.csv")

#Let's ditch those!
data<-filter(data, `Sequencing project`!="RTL 6997240")

data %>% group_by(`Sampling locality (water body, drainage, region)`, Site, Region) %>% 
  summarize(Count=n()) %>% ungroup() %>%
  select(`Sampling locality (water body, drainage, region)`, Region, Site, Count)

data %>% ungroup() %>% group_by(`Sequencing project`) %>% summarize(Count=n())


```

443 samples from 20 places it looks like.

What about a geographic distribution?

```{r, warning=FALSE, message=FALSE}
plot<-data %>% filter(Latitude != "NA") %>% group_by(Site, Latitude, Longitude) %>% summarize(SampleSize=n()) %>% select(Site, Latitude, Longitude, SampleSize) %>%
  unique()

alaska<-map_data("world") %>% filter(region %in% c("USA", "Canada"))

ggplot()+geom_polygon(data=alaska, aes(x=long, y=lat, group=group))+
  geom_point(data=plot, aes(x=Longitude, y=Latitude, size=SampleSize),
             fill="blue", alpha=0.75, pch=21)+
  xlab("Longitude")+
  ylab("Latitude")+
  coord_fixed(1.3, xlim=c(-180,-90), ylim=c(45,80))
 
```

Otter Lake previously had some problems. Now fixed.

```{r, warning=FALSE, message=FALSE}
otter<-data %>% filter(Latitude != "NA") %>% group_by(Site, Latitude, Longitude) %>% summarize(SampleSize=n()) %>% select(Site, Latitude, Longitude, SampleSize) %>%
  unique() %>% filter(Site=="OtterLake")
ggplot(otter)+geom_point(aes(x=Longitude, y=Latitude, size=SampleSize))
```

## Let's downsize the representation of South Central.

Let's get 20 MatSu/Anchorage and 15 Kenai

```{r, warning=FALSE, message=FALSE}
kenai <- filter(plot, Site %in% c("StormyLake", "TinyLake"))
anch<-filter(data, Region=="Southcentral") %>% filter(!(Site %in% c("StormyLake", "TinyLake"))) %>% filter(Latitude != "NA") %>% group_by(Site, Latitude, Longitude) %>% summarize(SampleSize=n()) %>% select(Site, Latitude, Longitude, SampleSize) %>%
  unique()
  

ggplot()+geom_polygon(data=alaska, aes(x=long, y=lat, group=group))+
  geom_point(data=kenai, aes(x=Longitude, y=Latitude, size=SampleSize),
             fill="blue", alpha=0.75, pch=21)+
   geom_point(data=anch, aes(x=Longitude, y=Latitude, size=SampleSize),
             fill="red", alpha=0.75, pch=21)+
  xlab("Longitude")+
  ylab("Latitude")+
  coord_fixed(1.3, xlim=c(-160,-140), ylim=c(60,65))
```

For lack of a better idea, I'll randomly sample these guys. 
eval = FALSE to prevent creating a new random set of samples.
```{r, message=FALSE, warning=FALSE, eval=FALSE}
kenaiSamples<-data %>% filter(Site %in% c("StormyLake", "TinyLake")) %>% sample_n(15) 
anchorageSamples<- data %>% filter(Region=="Southcentral") %>% 
  filter(!(Site %in% c("StormyLake", "TinyLake"))) %>% sample_n(20) 

natural<-data %>% filter(Region != "Southcentral")
samples<-rbind(rbind(kenaiSamples, anchorageSamples), natural)

#Many of these don't have lat/long data.
write_csv(samples, path="outputs/101/samples-to-analyze.csv")
```

```{r, message=FALSE, warning=FALSE, eval=FALSE}
save(samples, file="./outputs/101/samples-to-analyze.rda")
```

### Let's figure out which *.bams we need.
```{r, message=FALSE, warning=FALSE}
load(file="./outputs/101/samples-to-analyze.rda")
samples<-as_tibble(samples)
```


BK1703222 has samples like I-025_S200_sorted.bam
AN1711171 has samples like W-006_sorted.bam

Making a path like
/data/BK1703222/I-025_S200_sorted.bam
```{r, message=FALSE, warning=FALSE}
bamlist<-samples %>% select(`Sequencing project`, `RAD identifier`, Region, Site) %>% separate(`Sequencing project`, c("GW", "Project")) %>% mutate(String=paste("data/",Project,"/",`RAD identifier`,"_sorted.bam", sep=""))

write(bamlist$String, file="bamlists/197.txt")
save(bamlist, file="./outputs/101/197-with-Region-Site-BAMInfo.rda")
```


### Now, let's characterize average coverage. Hmmm.... May take a long time like this, but I have other things to entertain me, hahaha.

```{sh, eval=FALSE}
#samtools depth data/BK1703222/D-002_S67_sorted.bam | head -n 1000 | awk '{sum+=$3} END { print sum/NR}'
#cat bamlists/test.txt | while read line; do samtools depth $line | head -n 1000 | awk '{sum+=$3} END { print sum/NR}' >> bamlists/test.cov; done;
cat bamlists/test.txt | while read line; do samtools depth $line | awk '{sum+=$3} END { print sum/NR}' >> bamlists/test.cov; done;
srun -p t1small -t 8:00:00 --mem=8G --nodes=1 cat bamlists/197.txt  | while read line; do echo $line; samtools depth $line | awk '{sum+=$3} END { print sum/NR}' >> bamlists/197.cov; done;
```

Now we can read in the coverage and create a histogram, eventually removing samples with low coverage.
This coverage was not recreated exactly, and a different 175 was generated.
```{r, warning=FALSE, message=FALSE, warning=FALSE}
coverage<-read_tsv("bamlists/197.cov", col_names = FALSE)

samples$Coverage<-coverage$X1

ggplot(samples)+
  geom_histogram(aes(x=Coverage))

ggplot(samples)+
  geom_histogram(aes(x=Coverage))+
  facet_grid(.~Region)
```

```{r, warning=FALSE, message=FALSE}

#Removing two "odd" samples, D-026_R1_001.fastq.gz, E-026_R1_001.fastq.gz

filtered<- samples %>% filter(`Sequencing file name` != ("D-026_R1_001.fastq.gz")) %>%
                       filter(`Sequencing file name` != ("E-026_R1_001.fastq.gz")) %>%
                       top_frac(.90, Coverage)

filtered %>% group_by(Region) %>% summarize(Count=n())

filteredbams<-filtered %>% select(`Sequencing project`, `RAD identifier`, Region, Site) %>% separate(`Sequencing project`, c("GW", "Project")) %>% mutate(String=paste("data/",Project,"/",`RAD identifier`,"_sorted.bam", sep=""))

write(filteredbams$String, file="bamlists/175.txt")
write(filteredbams$String, file="bamlists/175.bamlist")
```