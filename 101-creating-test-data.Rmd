---
title: "Creating a test data set"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(maps)
library(mapdata)
```

## What do our samples look like?

```{r, warning=FALSE, message=FALSE}
data<-read_csv(file="./metadata/190828_Pike ddRAD data identifiers.csv")
data<-data %>% separate(`Coordinates (if available)`, into=c("Latitude", "Longitude"), sep =",")
data$Latitude<-as.numeric(data$Latitude)
data$Longitude<-as.numeric(data$Longitude)
nrow(data)
data %>% group_by(`Sampling locality (water body, drainage, region)`, Site, Region) %>% 
  summarize(Count=n()) %>% ungroup() %>%
  select(`Sampling locality (water body, drainage, region)`, Region, Site, Count)
```

510 samples from 20 places it looks like.

What about a geographic distribution?

```{r, warning=FALSE, message=FALSE}
plot<-data %>% filter(Latitude != "NA") %>% group_by(Site, Latitude, Longitude) %>% summarize(SampleSize=n()) %>% select(Site, Latitude, Longitude, SampleSize) %>%
  unique()

alaska<-map_data("world") %>% filter(region %in% c("USA", "Canada"))

ggplot()+geom_polygon(data=alaska, aes(x=long, y=lat, group=group))+
  geom_point(data=plot, aes(x=Longitude, y=Latitude, size=SampleSize),
             fill="blue", alpha=0.75, pch=21)+
  xlab("Longitude")+
  ylab("Latitude")+
  coord_fixed(1.3, xlim=c(-180,-90), ylim=c(45,80))
 
```

Otter Lake previously had some problems. Now fixed.

```{r, warning=FALSE, message=FALSE}
otter<-data %>% filter(Latitude != "NA") %>% group_by(Site, Latitude, Longitude) %>% summarize(SampleSize=n()) %>% select(Site, Latitude, Longitude, SampleSize) %>%
  unique() %>% filter(Site=="OtterLake")
ggplot(otter)+geom_point(aes(x=Longitude, y=Latitude, size=SampleSize))
```

## Let's downsize the representation of South Central.

Let's get 20 MatSu/Anchorage and 15 Kenai

```{r, warning=FALSE, message=FALSE}
kenai <- filter(plot, Site %in% c("StormyLake", "TinyLake"))
anch<-filter(data, Region=="Southcentral") %>% filter(!(Site %in% c("StormyLake", "TinyLake"))) %>% filter(Latitude != "NA") %>% group_by(Site, Latitude, Longitude) %>% summarize(SampleSize=n()) %>% select(Site, Latitude, Longitude, SampleSize) %>%
  unique()
  

ggplot()+geom_polygon(data=alaska, aes(x=long, y=lat, group=group))+
  geom_point(data=kenai, aes(x=Longitude, y=Latitude, size=SampleSize),
             fill="blue", alpha=0.75, pch=21)+
   geom_point(data=anch, aes(x=Longitude, y=Latitude, size=SampleSize),
             fill="red", alpha=0.75, pch=21)+
  xlab("Longitude")+
  ylab("Latitude")+
  coord_fixed(1.3, xlim=c(-160,-140), ylim=c(60,65))
```

For lack of a better idea, I'll randomly sample these guys. Looks like there are 302 "Southcentral" samples. Figuring like this: 510-302+15+20=243!

```{r, message=FALSE, warning=FAlSE, eval=FALSE}
kenaiSamples<-data %>% filter(Site %in% c("StormyLake", "TinyLake")) %>% sample_n(15) 
anchorageSamples<- data %>% filter(Region=="Southcentral") %>% 
  filter(!(Site %in% c("StormyLake", "TinyLake"))) %>% sample_n(20) 

natural<-data %>% filter(Region != "Southcentral")
samples<-rbind(rbind(kenaiSamples, anchorageSamples), natural)

#Many of these don't have lat/long data.
write_csv(samples, path="./101/samples-to-analyze.csv")
```

