---
title: "1200-more-samples"
author: "Mac Campbell"
date: "June 10, 2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning=FALSE,message=FALSE)
```

```{r}
library(tidyverse)
library(maps)
library(mapdata)
```

##More samples!!
Not downsampling southcentral fish. Copying 101-creating-test-data.Rmd at first.     

Note: Providing lat/longs for sampling locations that are empty. No idea where North Slope are from, using  69.900517°-154.645600°; Pooling collections from the Antlin (recall two different locations?)
Read in data.

There were two main sequencing efforts by RTL and GW. Chase says we should be suspicious of the RTL set.

There are therefore duplicated sequences.

```{r, warning=FALSE, message=FALSE}
data<-read_csv(file="./metadata/pike-meta-06102021.csv")
data<-data %>% separate(`Coordinates (if available)`, into=c("Latitude", "Longitude"), sep =",")
data$Latitude<-as.numeric(data$Latitude)
data$Longitude<-as.numeric(data$Longitude)

dups<-data %>% select(`Tissue identifier`) %>% group_by(`Tissue identifier`) %>% summarize(Count=n()) %>% filter (Count > 1)
sum(dups$Count)

#Make a df without the dups
dedup<- filter(data, !(`Tissue identifier` %in% dups$`Tissue identifier`))

temp<-filter(data, `Tissue identifier` %in% dups$`Tissue identifier`) %>% arrange(`DNA identifier`)

#Apparently we may not want the RTL files
temp2 <- temp %>% filter(`Sequencing project` !="RTL 6997240")

#And some are duplicated as "topoff"" files
temp3 <- temp2 %>% filter(!grepl("topoff",`Sequencing file name`))

#Recombine the df without dups and the temp file.
data<-rbind(dedup, temp3)

```



```{r, warning=FALSE, message=FALSE}
nrow(data)
#Are there sames that are not duplicated from the RTL sequencing effort?
RTL<-filter(data, `Sequencing project`=="RTL 6997240")
write_csv(RTL, path="outputs/101/RTL-unique-samples.csv")

#Let's ditch those!
data<-filter(data, `Sequencing project`!="RTL 6997240")

data %>% group_by(`Sampling locality (water body, drainage, region)`, Site, Region) %>% 
  summarize(Count=n()) %>% ungroup() %>%
  select(`Sampling locality (water body, drainage, region)`, Region, Site, Count)

data %>% ungroup() %>% group_by(`Sequencing project`) %>% summarize(Count=n())


```


What about a geographic distribution?

```{r, warning=FALSE, message=FALSE}
plot<-data %>% filter(Latitude != "NA") %>% group_by(Site, Latitude, Longitude) %>% summarize(SampleSize=n()) %>% select(Site, Latitude, Longitude, SampleSize) %>%
  unique()

alaska<-map_data("world") %>% filter(region %in% c("USA", "Canada"))

ggplot()+geom_polygon(data=alaska, aes(x=long, y=lat, group=group))+
  geom_point(data=plot, aes(x=Longitude, y=Latitude, size=SampleSize),
             fill="blue", alpha=0.75, pch=21)+
  xlab("Longitude")+
  ylab("Latitude")+
  coord_fixed(1.3, xlim=c(-180,-90), ylim=c(45,80))
 
```



Summarize:

```{r}
data %>% group_by(Site, Latitude, Longitude) %>% summarize(Count=n())
```

We may want to pool the samples that have multiple collections in one area, e.g. Lake Clark

##Create a list of files to calculate coverage from
```{r}
bamlist<-data %>%  separate(`Sequencing project`, c("GW", "Project")) %>% mutate(String=paste("data/",Project,"/",`RAD identifier`,"_sorted.bam", sep=""))

write(bamlist$String, file="bamlists/400.bamlist")
```

Now to calculate coverage and join that onto "bamlist"
On Chinook:     
```{sh, eval=FALSE}
srun -p t1small -t 8:00:00 --mem=8G --nodes=1 cat bamlists/400.txt  | while read line; do echo $line; samtools depth $line | awk '{sum+=$3} END { print sum/NR}' >> bamlists/400.cov; done;

```